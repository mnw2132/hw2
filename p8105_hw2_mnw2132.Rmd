---
title: "p8105_hw2_mnw2132"
author: "Mary Williams"
date: "2024-09-30"
output: github_document
---

## Problem 1

*Bring data in*
```{r}
library(tidyverse)
transit_data = read_csv(file = "/Users/nickywilliams/Desktop/hw2/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na=c(".","NA","")) %>%
  janitor::clean_names()
transit_data
select(transit_data, line, station_location, station_name, station_latitude,  station_longitude, starts_with("route"), entry, vending, entrance_type, ada) %>%

mutate(
  entry = case_match(
    entry,
    "YES" ~ 1,
    "NO" ~ 0))
```
*Read and clean data *
```{r}
transit_data
transit_data_clean=
select(transit_data, line, station_location, station_name, station_latitude,  station_longitude, starts_with("route"), entry, vending, entrance_type, ada) %>%

mutate(
  entry = case_match(
    entry,
    "YES" ~ 1,
    "NO" ~ 0))
```

*Paragraph about the data* 

After cleaning the data, it contains 20 variables, including transit_data, line, station_location, station_name, station_latitude,  station_longitude, entry, vending, entrance_type, ada, and all 11 variables starting with route. We cleaned the data by making all missing data represented by "na", putting all names into snake format, selecting the 20 variables to keep within the data, and by changing the entry variable to a logical variable where 1=yes and 0=no. There 1,868 rows and 20 columns in the resulting data set. Considering that the columns are variables, the rows are observations and every value has a cell, this data is tidy. 

*How many stations*
```{r}
number_stations=
  distinct(transit_data_clean, station_name, line)
number_stations
nrow(number_stations)
```
Therefore, there are 465 stations in nyc. 

*How many stations are ADA compliant*
```{r}
ADA_stations <- transit_data_clean |>
    filter(ada=="TRUE") |>
    distinct(station_name, line) |>
nrow()

ADA_stations
```
There are 84 ADA compliant stations. 

*What proportion of entrances/exits w/out vending allow entrance*
```{r}
no_vending_entry <- transit_data %>%
  filter(vending == "NO") %>%
  summarise(proportion = mean(entry == "YES"))

no_vending_entry

```
The proportion is 37.7%. 

## NEED TO FINISH THIS PROBLEM

## Problem 2
```{r}
library(readxl)
library(tidyverse)
#Mr. Trash Wheel
Mr_trash_wheel = read_excel("/Users/nickywilliams/Desktop/hw2/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel") %>%
  janitor::clean_names() %>%
select(-month, -year, -date) %>%
  mutate(sports_balls = as.integer(round(sports_balls)))
```

```{r}
#Professor Trash Wheel
Professor_Trash_Wheel = read_excel("/Users/nickywilliams/Desktop/hw2/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel") %>%
  janitor::clean_names() %>%
select (-month, -year)
```

```{r}
#Gwynnda
Gwynnda_Trash_Wheel = read_excel("/Users/nickywilliams/Desktop/hw2/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel") %>%
  janitor::clean_names() %>%
select (-month, -year) 
```

```{r}
#Combine
tidy_trash=
  bind_rows(Mr_trash_wheel, Professor_Trash_Wheel, Gwynnda_Trash_Wheel)

tidy_trash
```


In this, there are `r nrow(tidy_trash)` by `r ncol(tidy_trash)` columns. Some of the key variables include dumpster number, weight in tons, and volume in cubic yards. For available data, the total weight of the trash collected by Professor Trash Wheel was `r sum(Professor_Trash_Wheel$weight_tons, na.rm=TRUE)` tons. Meanwhile, the number of cigarette butts Gwynnda collected in June 2022 was `r sum(Gwynnda_Trash_Wheel$cigarette_butts[Gwynnda_Trash_Wheel$date >= as.Date("2022-06-01") & Gwynnda_Trash_Wheel$date <= as.Date("2022-06-30")], na.rm=TRUE)`.

##Problem 3
```{r}
#import and format bakers data
bakers = read_csv(file = "/Users/nickywilliams/Desktop/hw2/gbb_datasets/bakers.csv") %>%
  janitor::clean_names()
  na.omit(bakers)
```

```{r}
#import and format bakes data
bakes = read_csv(file = "/Users/nickywilliams/Desktop/hw2/gbb_datasets/bakes.csv") %>%
  janitor::clean_names()
  na.omit(bakes)
```

```{r}
#import and format results data 
results = read_csv(file = "/Users/nickywilliams/Desktop/hw2/gbb_datasets/results.csv", skip=2) %>%
  janitor::clean_names()
  na.omit(results)
```
```{r}
#check column names
colnames(bakers)
colnames(bakes)
colnames(results)

#Make sure the columns names are both baker
colnames(bakers)[colnames(bakers) == "baker_name"] = "baker"

#Check that bakers and results have "baker" column
colnames(bakers)
nrow(bakers)
colnames(bakes)
nrow(bakes)
colnames(results)
nrow(results)

#check how many results are not in baker 
anti_tidy_trash = anti_join(results, bakers, by = c("baker"))

all_tidy_trash = anti_join(anti_tidy_trash, bakes, by = c("baker"))

#Export as csv
write_csv(all_tidy_trash, "all_tidy_trash.csv")

colnames(all_tidy_trash)
```

During the data cleaning process, I imported each data set, cleaned each set with "janitor::clean_names()" take format the titles as snake format, and did "na.omit(...)" to remove any missing data. For results I also did "skip=2" to remove the 1st too rows of notes, which made the column titles the same as the other files. For bakers, I also had to change the column name "baker_name" to "baker" so that when they are joined, the column "baker" is same within all of the datasets. Considering that I could not join 3 datasets all together directly, I joined results and bakers, and then combined bakes into the combined file. 

The final data set includes `r nrow(all_tidy_trash)` rows and `r ncol(all_tidy_trash)` columns and has each person's information lined up according to each season and episode they were in. 

```{r}
table_all_trash = all_tidy_trash %>%
  filter(series >= 5 & series <= 10) %>%
  filter(result %in% c( "WINNER", "STAR BAKER")) %>%
 distinct()
```

```{r}
print(table_all_trash)
```

```{r}
#viewership
viewers <- read_csv("/Users/nickywilliams/Desktop/hw2/gbb_datasets/viewers.csv") %>%
  janitor::clean_names()
  na.omit(results)

# Show the first 10 rows
head(viewers, 10)

colnames(viewers)
season_1 <- viewers %>%
  summarize(season_1 = mean(`series_1`, na.rm = TRUE))

season_5 <- viewers %>%
  summarise(season_5 = mean(`series_5`, na.rm = TRUE))

print(season_1)
print(season_5)
```

The average viewership in season 1 was `print(season_1)` and for season 5 it was `print(season_5)`. 
